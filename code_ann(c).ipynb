{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRDh6OqztMDTKpmIO5BVqk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanju0392/ANN-CLASSIFICATION-/blob/main/code_ann(c).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "SIspSeO8tf6d"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights with random values\n",
        "def weights(input_size, hidden_size, output_size):\n",
        "   W1 = np.random.randn(input_size, hidden_size)\n",
        "   b1 = np.random.randn(1, hidden_size)\n",
        "   W2 = np.random.randn(hidden_size, output_size)\n",
        "   b2 = np.random.randn(1, output_size)\n",
        "   return W1,b1,W2,b2"
      ],
      "metadata": {
        "id": "LJmvEwbtASAo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "function for different activation functions"
      ],
      "metadata": {
        "id": "SzoRrPGZ6_L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def activationfunction(activationfunction):\n",
        "     def sigmoid(x):\n",
        "       return 1 / (1 + np.exp(-x))\n",
        "     def relu(x):\n",
        "       return np.maximum(0, x)\n",
        "     def leaky_relu(x, alpha=0.01):\n",
        "       return np.maximum(alpha * x, x)\n",
        "     def tanh(x):\n",
        "       return np.tanh(x)\n",
        "     def softmax(x):\n",
        "       exp_x = np.exp(x - np.max(x))\n",
        "       return exp_x / np.sum(exp_x)\n",
        "     if activationfunction == sigmoid:\n",
        "        return sigmoid  \n",
        "     elif activationfunction == relu:\n",
        "        return relu\n",
        "     elif activationfunction == leaky_relu:\n",
        "        return leaky_relu\n",
        "     elif activationfunction == tanh:\n",
        "        return tanh\n",
        "     else:  \n",
        "        return softmax\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QhxutRnW4gOX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "function for forward propogation"
      ],
      "metadata": {
        "id": "Odfn1lFRuztg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(X):\n",
        "        # Forward pass through the network\n",
        "        z1 = np.dot(X,W1) + b1\n",
        "        a1 = activationfunction(relu(z1)) # ReLU activation function\n",
        "        z2 = np.dot(X,W2) + b2\n",
        "        y_hat = activationfunction(sigmoid(z2))\n",
        "        return z1,a1,z2,y_hat"
      ],
      "metadata": {
        "id": "QDoXI1yBoAql"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "functions for different activation function derivatives"
      ],
      "metadata": {
        "id": "7djkZh7w84ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def derivativefunction(activationfunction):\n",
        "     def sigmoid_derivative(x):\n",
        "       return sigmoid(x) * (1 - sigmoid(x))\n",
        "     def tanh_derivative(x):\n",
        "       return 1 - np.tanh(x)**2\n",
        "     def relu_derivative(x):\n",
        "       return np.where(x > 0, 1, 0)\n",
        "     def leaky_relu_derivative(x, alpha=0.01):\n",
        "       dx = np.ones_like(x)\n",
        "       dx[x < 0] = alpha\n",
        "       return dx\n",
        "     def softmax_derivative(x):\n",
        "       p = softmax(x)\n",
        "       return np.diag(p) - np.outer(p, p)  \n",
        "     if derivativefunction == sigmoid_derivative:\n",
        "        return sigmoid_derivative  \n",
        "     elif derivativefunction == relu_derivative:\n",
        "        return relu_derivative\n",
        "     elif derivativefunction == leaky_relu_derivative:\n",
        "        return leaky_relu_derivative\n",
        "     elif derivativefunction == tanh_derivative:\n",
        "        return tanh_derivative\n",
        "     else:  \n",
        "        return softmax_derivative"
      ],
      "metadata": {
        "id": "OJm9Z21v83M_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "function for backward propogation"
      ],
      "metadata": {
        "id": "ZBRYXkwivyA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward(X, y, learning_rate):\n",
        "  # Backward pass to update weights and biases\n",
        "  m = X.shape[0]\n",
        "  dZ2 = y_hat - y\n",
        "  dW2 = np.dot(a1.T, dZ2) / m\n",
        "  db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "  dA1 = np.dot(dZ2, W2.T)\n",
        "  dZ1 = dA1 * (z1 > 0)\n",
        "  dW1 = np.dot(X.T, dZ1) / m\n",
        "  db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "        \n",
        "  # Update weights and biases\n",
        "  W2  = W2- learning_rate * dW2\n",
        "  b2  = b2 -learning_rate * db2\n",
        "  W1  = W1 -learning_rate * dW1\n",
        "  b1  = b1 -learning_rate * db1\n",
        "  return W2,b2,W1,b2"
      ],
      "metadata": {
        "id": "4kkEFrEHvxU4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X, y, learning_rate, epochs):\n",
        "# Train the neural network\n",
        "  for i in range(epochs):\n",
        "    forward(X)\n",
        "    backward(X, y, learning_rate)"
      ],
      "metadata": {
        "id": "mjiXuif4p_Ca"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " \n",
        "def predict(X):\n",
        "  # Make predictions using the trained model\n",
        "  forward(X)\n",
        "  return np.argmax(y_hat, axis=1)"
      ],
      "metadata": {
        "id": "U7idDFHx2pNu"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}